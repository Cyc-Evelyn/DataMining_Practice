{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "「2020.07.13_PM2.5linear.ipynb」的副本",
      "provenance": [],
      "authorship_tag": "ABX9TyNI/c1bAj+O8XMqghuBKI6/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnR5vpgKzKal"
      },
      "source": [
        " import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_temp = [] \n",
        "test_temp = []\n",
        "#共有18種汙染物,先做出18個容器作為分類用\n",
        "for i in range(18):\n",
        "    train_temp.append([])\n",
        "    test_temp.append([])\n",
        "\n",
        "temp = pd.read_excel('107年新竹站_20190315.xlsx')\n",
        "\"\"\"\n",
        "資料前處理\n",
        "\"\"\"    \n",
        "\n",
        "'''\n",
        "以下是將特殊符號或是空值用0替代，但本次作業用前後兩筆資料取平均代替\n",
        "\n",
        "temp.replace(['[-+]?([0-9]*[\\.]?[0-9]+[#*x])'] ,0 , regex=True , inplace=True) \n",
        "#正規表達式 +*? 取特殊符號(#*x)的資料replace為0\n",
        "temp = temp.fillna(0) \n",
        "#空值以0取代\n",
        "'''\n",
        "\n",
        "temp = temp.replace(\"NR\",0) \n",
        "#NR表示無降雨，以0替代\n",
        "\n",
        "\"\"\"\n",
        "時間序列shift用\n",
        "\"\"\"        \n",
        "def shift(l, n): \n",
        "    return l[n:]\n",
        "\n",
        "#l為list名稱,n為移動多少\n",
        "#利用list性質,做出該效果\n",
        "#eg. l=[0,1,2,3,4]  \n",
        "#=l=shift(l,2)轉換為由2開始 l=[2,3,4],達成時間往前推移兩格的效果(由第2個開始取)\n",
        "#l=shift(l,-1) 轉換為由倒數第一個開始取 l=[4]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LSw6jDqEXwF"
      },
      "source": [
        "\"\"\"\n",
        "將資料以時間分成train跟test資料\n",
        "#train = temp[temp[\"日期\"].between('2018/10/01','2018/11/30')]                \n",
        "#test = temp[temp[\"日期\"] >= '2018/12/01']\n",
        "\n",
        "train data 2個月，2018/10/01到2018/11/30\n",
        "test data 1個月，2018/12/01-2018/12/31\n",
        "將train及test data中的汙染物數值用時序連接在一起\n",
        "例如：一列即為10/1-11/30的所有資料\n",
        "\"\"\"\n",
        "\n",
        "#,行---列|||,i行j列\n",
        "for i in range(0,len(temp)):\n",
        "    if temp.iloc[i,0] >= '2018/10/01' and temp.iloc[i,0] <= '2018/11/30': #由i(行)的第0列開始搜尋符合條件的資料,也就是每行的0列\n",
        "        #第3-27列是汙染物值,取3-27列\n",
        "        for j in range(3,27): \n",
        "            train_temp[i%18].append((temp.iloc[i,j]))#[i%18]是把資料分成18堆(時序資料會自動分類),但為啥結果是從第7種汙染源開始?\n",
        "    elif temp.iloc[i,0] >= '2018/12/01':\n",
        "        for j in range(3,27):\n",
        "            test_temp[i%18].append((temp.iloc[i,j]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YtDhHRhxzlVE"
      },
      "source": [
        "\"\"\"\n",
        "train data，只拿PM2.5值\n",
        "\"\"\"\n",
        "\n",
        "#第4行的汙染物是PM2.5 (由NOx開始)\n",
        "#len(train_PM25_temp) \n",
        "train_PM25_temp =  train_temp[3]\n",
        "\n",
        "#處理異常值及空值，將其轉為前後項的平均\n",
        "for i in range(len(train_PM25_temp)):\n",
        "    #處理異常值及空值，將其轉為前後項的平均\n",
        "    j = i-1\n",
        "    k = i+1\n",
        "    #使用find函數:若找到目標則回傳目標物,都沒找到則回傳-1\n",
        "    #當有i是異常值:\n",
        "    if str(train_PM25_temp[i]).find(\"*\") != -1 or str(train_PM25_temp[i]).find(\"#\") != -1 or str(train_PM25_temp[i]).find(\"A\") != -1 or np.isnan(train_PM25_temp[i]) == True:\n",
        "          #若i的前項(j)是異常值,就用j的前一項(j-1)替代j\n",
        "          while str(train_PM25_temp[j]).find(\"*\") != -1 or str(train_PM25_temp[j]).find(\"#\") != -1 or str(train_PM25_temp[j]).find(\"A\") != -1 or np.isnan(train_PM25_temp[j]) == True:\n",
        "                   j = j-1\n",
        "          #若i的後項(k)是異常值,就用k的後一項(k+1)替代k\n",
        "          while str(train_PM25_temp[k]).find(\"*\") != -1 or str(train_PM25_temp[k]).find(\"#\") != -1 or str(train_PM25_temp[k]).find(\"A\") != -1 or np.isnan(train_PM25_temp[k]) == True:\n",
        "                   k = k+1\n",
        "          #確認沒問題後做平均處理得到i\n",
        "          train_PM25_temp[i] = (train_PM25_temp[j] + train_PM25_temp[k]) / 2\n",
        "train_X = []\n",
        "train_y = []\n",
        "period = 6\n",
        "#用六小時的資料去預測第七小時的PM2.5\n",
        "length = len(train_PM25_temp) - period\n",
        "for i in range(length):\n",
        "    if len(train_PM25_temp) > period:\n",
        "        train_X.append(train_PM25_temp[0:period]) #x為1-6小時的資料\n",
        "        train_y.append(train_PM25_temp[6]) #y為第7小時的資料(也就是正確答案)\n",
        "        train_PM25_temp = shift(train_PM25_temp,1)\n",
        "        #用shift把資料往前補，方便固定前0-5欄位都是x,第6欄是y "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8h1IGtlczyZ7"
      },
      "source": [
        "\"\"\"\n",
        "#test data，只拿PM2.5值\n",
        "\"\"\"\n",
        "#第4個欄位是PM2.5\n",
        "test_PM25_temp =  test_temp[3]#抽取pm2.5的部分做檢驗(為第三個list)\n",
        "#異常值處理同上\n",
        "for i in range(len(test_PM25_temp)):\n",
        "    j = i-1\n",
        "    k = i+1\n",
        "    if str(test_PM25_temp[i]).find(\"*\") != -1 or str(test_PM25_temp[i]).find(\"#\") != -1 or str(test_PM25_temp[i]).find(\"A\") != -1 or np.isnan(test_PM25_temp[i]) == True:\n",
        "          while str(test_PM25_temp[j]).find(\"*\") != -1 or str(test_PM25_temp[j]).find(\"#\") != -1 or str(test_PM25_temp[j]).find(\"A\") != -1 or np.isnan(test_PM25_temp[j]) == True:\n",
        "                   j = j-1\n",
        "          while str(test_PM25_temp[k]).find(\"*\") != -1 or str(test_PM25_temp[k]).find(\"#\") != -1 or str(test_PM25_temp[k]).find(\"A\") != -1 or np.isnan(test_PM25_temp[k]) == True:\n",
        "                   k = k+1\n",
        "          test_PM25_temp[i] = (test_PM25_temp[j] + test_PM25_temp[k]) / 2\n",
        "          \n",
        "test_X = []\n",
        "test_y = []\n",
        "period = 6\n",
        "length = len(test_PM25_temp) - period\n",
        "#同上\n",
        "for i in range(length):\n",
        "    if len(test_PM25_temp) > period:\n",
        "        test_X.append(test_PM25_temp[0:period])\n",
        "        test_y.append(test_PM25_temp[6])\n",
        "        test_PM25_temp = shift(test_PM25_temp,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNRbkJ3qFAQV"
      },
      "source": [
        "##linear regression復刻版-簡單linear\n",
        "[來源](https://www.twblogs.net/a/5bd33c3c2b717778ac1ff843"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExgUJNE8IzIt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "df436302-5530-47ea-dea6-d3146e93e6a5"
      },
      "source": [
        "#前兩行==>生成數個b截距與w權重,並以他們做不同組合求最小loss\n",
        "\n",
        "'''\n",
        "#0.0855=171/2000,原為使用-+0.0855step0.00005生成3420個b的值(反推為171/2000*x*2=3420,x=20000,step=1/20000)\n",
        "#3420怎麼來的(228天*15組=3420組數據)\n",
        "思路：利用前9個小時的PM2.5的數據來預測下一個小時的PM2.5數據，這樣一天的數據可以劃分爲15組，去除掉PM2.5小於0的無效觀測天數據\n",
        "由於我們有做資料前處理,皆為有效數據,2018/10/01到2018/11/30有61天*17=1037組\n",
        "#應使用(-0.25925, 0.25925, step=0.0005)(跑不動我縮小作者倍率模擬一下流程)\n",
        "\n",
        "#0.07695=1539/20000,原為使用step0.000005生成30780組w的值，每組w有9個值w0-w8\n",
        "#改成1037組(一組6個權重w),(-0.15555, 0.15555, step=0.0005)\n",
        "'''\n",
        "\n",
        "X = np.arange(-0.0855, 0.0855, step=0.005)\n",
        "Y = np.arange(-0.07695, 0.07695, step=0.0005)\n",
        "\n",
        "Loss = np.zeros((len(X), len(X)))\n",
        "# z是一個值全爲0，len(X)行len(X)列的矩陣,即3420組w*3420組b的不同組合，每種組合都計算一下它們的loss值（樣本爲全體數據）\n",
        "for i in range(len(X)):\n",
        "\tfor j in range(len(X)):\n",
        "\t\t# 雙層for循環，手動計算方差和的平均值\n",
        "\t\tb = X[i] #x[0],x[1]...\n",
        "\t\tw = Y[j * 6:j * 6 + 6] #y[0:6],y[1,7]...\n",
        "\t\tLoss[i][j] = 0\n",
        "\t\tfor index, x_temp in enumerate(train_X):\n",
        "            # 計算每種w和b的組合的loss值,結果y和預測值的平方差\n",
        "\t\t\tLoss[i][j] = Loss[i][j] + (\n",
        "\t\t\t\t\ttrain_y[index] - b - w[0] * x_temp[0] - w[1] * x_temp[1] - w[2] *\n",
        "\t\t\t\t\tx_temp[2] - w[3] * x_temp[3] - w[4] * x_temp[4]- w[5] * x_temp[5]) ** 2\n",
        "#方差和的平均值\n",
        "Loss[i][j] = Loss[i][j] / len(train_X)\n",
        "print(Loss[i][j]) #這就是mae吧??\n",
        "print(b, w)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "352.070673538176\n",
            "0.08450000000000014 [0.02505 0.02555 0.02605 0.02655 0.02705 0.02755]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cCmQQSDmQ_4"
      },
      "source": [
        "##使用BGD的linear\n",
        "\n",
        "---\n",
        "批量梯度下降:一次使用全部樣本來更新w跟b(參數)\n",
        "\n",
        "蜥蜴書p119\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMkD4Vc9zq4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a1bfecf2-d5dc-48bd-a545-ef063da588a0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# 定義loss函數,使用函數可以不斷自動更新(像是loop中loop的感覺?)\n",
        "# xdata即輸入的x訓練數據組，ydata即輸入的y訓練數據組，wdata即設定的w0-w8，bdata即設定的b\n",
        "def loss_function1(xdata, ydata, wdata, bdata):\n",
        "\tloss = 0\n",
        "\tfor n, x in enumerate(xdata):\n",
        "\t\t# 計算loss\n",
        "\t\tloss = loss + (\n",
        "\t\t\t\tydata[n] - bdata - wdata[0] * x[0] - wdata[1] * x[1] - wdata[2] *\n",
        "\t\t\t\tx[2] - wdata[3] * x[3] - wdata[4] * x[4]\n",
        "\t\t\t\t- wdata[5] * x[5]) ** 2\n",
        "\treturn loss / len(xdata)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 定義b的初始值 \n",
        "b = -0.08542942192781317\n",
        "# 定義w0-w5的初始值 \n",
        "w = [-0.33030855334867937, 0.05388424505307146, 0.5966401292268716, -0.6020419118650027, -0.28417190443282037,\n",
        "\t 1.3284417881100619]\n",
        "\n",
        "# 定義爲學習速率lr的初始值\n",
        "# lr的值設置可以參照我們模擬手動計算時設置的步長\n",
        "lrw, lrb = 0.0000000001, 0.000000001\n",
        "# 定義訓練次數iteration\n",
        "iteration = 1000\n",
        "# 定義b_history、w_history、loss_history用來存儲訓練過程中更新的w、b、和loss\n",
        "b_history = [b]\n",
        "w_history = [w]\n",
        "# loss_history初始值需要計算，用初始的w和b輸入函數loss_function1進行計算\n",
        "loss_history = [loss_function1(train_X, train_y, w, b)]\n",
        "\n",
        "# 定義w偏導數和b的偏導數初始值爲0\n",
        "b_grad = 0.0\n",
        "w_grad = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "\n",
        "# 定義更新w的偏導數、b的偏導數、w和b的函數 \n",
        "# w_grad即w的偏導數，b_grad即b的偏導數,xdata即輸入的x訓練數據組，ydata即輸入的y訓練數據組，wdata即設定的w0-w8，bdata即設定的b\n",
        "\n",
        "def update_b_grad_and_w_grad_and_w_and_b(bgrad, wgrad, xdata, ydata, wdata, bdata):\n",
        "\t# xdata的數據組規模數量表示了我們用多大規模的數據來更新一次w和b的偏導數，這就是BGD、SGD、MBGD之間的區別\n",
        "\tfor n, x in enumerate(xdata):\n",
        "\t\tbgrad = bgrad - 2.0 * (\n",
        "\t\t\t\tydata[n] - bdata - wdata[0] * x[0] - wdata[1] * x[1] - wdata[2] * x[2] - wdata[3] * x[3] - wdata[4] *\n",
        "\t\t\t\tx[4] - wdata[5] * x[5] ) * 1.0\n",
        "\t\tfor m in range(6):\n",
        "\t\t\twgrad[m] = wgrad[m] - 2.0 * (\n",
        "\t\t\t\t\tydata[n] - bdata - wdata[0] * x[0] - wdata[1] * x[1] - wdata[2] * x[2] - wdata[3] * x[3] - wdata[\n",
        "\t\t\t\t4] *x[4] - wdata[5] * x[5]) * x[m]\n",
        "\t# 用偏導數bgrad/wgrad乘以學習速率lr來更新w和b\n",
        "\tbdata = bdata - lrb * bgrad\n",
        "\tfor m in range(6):\t\n",
        "\t\twdata[m] = wdata[m] - lrw * w_grad[m]\n",
        "\treturn bdata, wdata\n",
        "\n",
        "\n",
        "# 存儲訓練得出的最好的w和b值，並記錄此時的training error\n",
        "b_train_best = 1\n",
        "w_train_best = []\n",
        "training_error = 10000\n",
        "'''\n",
        "每訓練一次存儲更新的w和b\n",
        "# 並且將更新的w和b的值賦值給w和b變量\n",
        "# 也就是說每訓練一次更新了4個變量：b_grad、w_grad、w、b，b_grad和w_grad直接在函數體內更新，w和b要記錄更新後的值，所以作爲函數返回值\n",
        "# 在下面這句更新\n",
        "'''\n",
        "for i in range(iteration):\n",
        "\tb_temp, w_temp = update_b_grad_and_w_grad_and_w_and_b(b_grad , w_grad, train_X, train_y, w, b)\n",
        "\tb, w = b_temp, w_temp #更新\n",
        "\tloss_history.append(loss_function1(train_X, train_y, w_temp, b_temp))\n",
        "\tif loss_history[-1] < training_error:\n",
        "\t\ttraining_error = loss_history[-1]\n",
        "\t\tb_train_best = b_temp\n",
        "\t\tw_train_best.clear()\n",
        "\t\t[w_train_best.append(i) for i in w_temp]\n",
        "\tb_history.append(b_temp)\n",
        "\tw_history.append(w_temp)\n",
        "# 得到訓練出來的最佳w、b、Loss\n",
        "print(training_error, b_train_best, w_train_best)\n",
        "testing_error = loss_function1(test_X, test_y, w_train_best, b_train_best)\n",
        "print(\"BGDlinear的MAE是{}\".format(testing_error))#mae\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.936221935727028 -0.08432999764210623 [0.06314656268580168, -0.08131408805172351, 0.32185961671058944, -0.24839935799261728, -0.2325949043397792, 1.1697895918974865]\n",
            "BGDlinear的MAE是12.64822230345141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sSC_2luuPWA"
      },
      "source": [
        "##使用SGD的linear\n",
        "\n",
        "\n",
        "---\n",
        "隨機梯度下降:一次使用一個樣本更新w跟b(有m個樣本就會迭代m次)\n",
        "\n",
        "蜥蜴書p121\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WzRxAejwg4U7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "43b133d1-673c-4748-e737-e4af5f6b339b"
      },
      "source": [
        "def loss_function1(xdata, ydata, wdata, bdata):\n",
        "\tloss = 0\n",
        "\tloss = loss + (\n",
        "\t\t\tydata - bdata - wdata[0] * xdata[0] - wdata[1] * xdata[1] - wdata[2] *\n",
        "\t\t\txdata[2] - wdata[3] * xdata[3] - wdata[4] * xdata[4]\n",
        "\t\t\t- wdata[5] * xdata[5]) ** 2\n",
        "\treturn loss\n",
        "\n",
        "\n",
        "# 函數返回方差和的平均值\n",
        "\n",
        " \n",
        "# 定義b的初始值\n",
        "b = -0.08287711928778152\n",
        "# 定義w0-w8的初始值\n",
        "w = [0.2336750995700671, 0.0011688047729486227, -0.07906800252618741, -0.2912006071443291, 0.23385921935943532,\n",
        "\t 0.8659526367683459]\n",
        "# 注意上面的初始值是我們模擬手動計算時找到的一組Loss值較小的值，比如這組的Loss值爲235.5967253011724\n",
        "# 定義爲學習速率lr的初始值\n",
        "# lr的值設置可以參照我們模擬手動計算時設置的步長\n",
        "lrw, lrb = 0.0000000001, 0.000000001\n",
        "# 定義訓練次數iteration\n",
        "iteration = 1000\n",
        "# 定義b_history、w_history、loss_history用來存儲訓練過程中更新的w、b、和loss\n",
        "b_history = [b]\n",
        "w_history = [w]\n",
        "# loss_history初始值需要計算，用初始的w和b輸入函數loss_function1進行計算\n",
        "loss_history = []\n",
        "\n",
        "# 定義w偏導數和b的偏導數初始值爲0\n",
        "b_grad = 0.0\n",
        "w_grad = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "\n",
        "# 定義更新w的偏導數、b的偏導數、w和b的函數\n",
        "# bgrad即w的偏導數，bgrad即b的偏導數，xdata即輸入的x訓練數據組，ydata即輸入的y訓練數據組，wdata即設定的w0-w8，bdata即設定的b\n",
        "def update_b_grad_and_w_grad_and_w_and_b(bgrad, wgrad, xdata, ydata, wdata, bdata):\n",
        "\t# 定義SGD方法的梯度下降優化器\n",
        "\tbgrad = bgrad - 2.0 * (\n",
        "\t\t\tydata - bdata - wdata[0] * xdata[0] - wdata[1] * xdata[1] - wdata[2] * xdata[2] - wdata[3] * xdata[3] -\n",
        "\t\t\twdata[4] *\n",
        "\t\t\txdata[4] - wdata[5] * xdata[5]) * 1.0\n",
        "\tfor m in range(6):\n",
        "\t\twgrad[m] = wgrad[m] - 2.0 * (\n",
        "\t\t\t\tydata - bdata - wdata[0] * xdata[0] - wdata[1] * xdata[1] - wdata[2] * xdata[2] - wdata[3] * xdata[3] -\n",
        "\t\t\t\twdata[4] * xdata[4] - wdata[5] * xdata[5]) * xdata[m]\n",
        "\t# 用偏導數乘以學習速率來更新w和b\n",
        "\tbdata = bdata - lrb * bgrad\n",
        "\tfor m in range(6):\n",
        "\t\twdata[m] = wdata[m] - lrw * w_grad[m]\n",
        "\treturn bdata, wdata\n",
        "\n",
        "\n",
        "# 存儲訓練得出的最好的w和b值，並記錄此時的training error\n",
        "b_train_best = 0\n",
        "w_train_best = []\n",
        "training_error = 10000\n",
        "\n",
        "for i in range(iteration):\n",
        "\tx_data_SGD = train_X[i]\n",
        "\ty_data_SGD = train_y[i]\n",
        "\tb_temp, w_temp = update_b_grad_and_w_grad_and_w_and_b(b_grad, w_grad, x_data_SGD, y_data_SGD, w, b)\n",
        "\t# 每訓練一次存儲更新的w和b\n",
        "\t# 並且將更新的w和b的值賦值給w和b變量\n",
        "\t# 也就是說每訓練一次更新了4個變量：b_grad、w_grad、w、b，b_grad和w_grad直接在函數體內更新，w和b要記錄更新後的值，所以作爲函數返回值\n",
        "\t# 在下面這句更新\n",
        "\tb, w = b_temp, w_temp\n",
        "\tloss_history.append(loss_function1(x_data_SGD, y_data_SGD, w_temp, b_temp))\n",
        "\tif loss_history[-1] < training_error:\n",
        "\t\ttraining_error = loss_history[-1]\n",
        "\t\tb_train_best = b_temp\n",
        "\t\tw_train_best.clear()\n",
        "\t\t[w_train_best.append(i) for i in w_temp]\n",
        "\tb_history.append(b_temp)\n",
        "\tw_history.append(w_temp)\n",
        "# 得到訓練出來的最佳w、b、Loss\n",
        "print(training_error, b_train_best, w_train_best)\n",
        "testing_error = 0\n",
        "for i in range(len(test_X)):\n",
        "\ttesting_error = testing_error + loss_function1(test_X[i], test_y[i], w_train_best, b_train_best)\n",
        "testing_error = testing_error / len(test_X)\n",
        "print(\"SGDlinear的MAE是{}\".format(testing_error))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0620307248927059e-05 -0.08287648265235675 [0.23386181575939752, 0.0013839533648000496, -0.07879383268728844, -0.29090315395689237, 0.23410200731620903, 0.8662277079908691]\n",
            "SGDlinear的MAE是17.29715712586813\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydanfaJEzG5r"
      },
      "source": [
        "##使用MBGD的linear\n",
        "\n",
        "---\n",
        "小批量梯度下降 :一次使用固定量的樣本數更新b跟w(不是使用全部樣本)\n",
        "\n",
        "蜥蜴書p124\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wRcTqWrzX25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c39bd7eb-36b4-4d8f-d3d3-88081427c423"
      },
      "source": [
        "def loss_function1(xdata, ydata, wdata, bdata):\n",
        "\tloss = 0\n",
        "\tfor n, x in enumerate(xdata):\n",
        "\t\t# 計算loss\n",
        "\t\tloss = loss + (\n",
        "\t\t\t\tydata[n] - bdata - wdata[0] * x[0] - wdata[1] * x[1] - wdata[2] *\n",
        "\t\t\t\tx[2] - wdata[3] * x[3] - wdata[4] * x[4]\n",
        "\t\t\t\t- wdata[5] * x[5]) ** 2\n",
        "\treturn loss / len(xdata)\n",
        "\n",
        "\n",
        "# 定義b的初始值\n",
        "b = -0.08287711928778152\n",
        "# 定義w0-w5的初始值\n",
        "w = [0.2336750995700671, 0.0011688047729486227, -0.07906800252618741, -0.2912006071443291, 0.23385921935943532,\n",
        "\t 0.8659526367683459]\n",
        "# 注意上面的初始值是我們模擬手動計算時找到的一組Loss值較小的值，比如這組的Loss值爲235.5967253011724\n",
        "# 定義爲學習速率lr的初始值\n",
        "# lr的值設置可以參照我們模擬手動計算時設置的步長\n",
        "lrw, lrb = 0.0000000001, 0.000000001\n",
        "# 定義訓練次數iteration\n",
        "iteration = 1000\n",
        "# 定義b_history、w_history、loss_history用來存儲訓練過程中更新的w、b、和loss\n",
        "b_history = [b]\n",
        "w_history = [w]\n",
        "# loss_history初始值需要計算，用初始的w和b輸入函數loss_function1進行計算\n",
        "loss_history = []\n",
        "\n",
        "# 定義w偏導數和b的偏導數初始值爲0\n",
        "b_grad = 0.0\n",
        "w_grad = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "\n",
        "# 定義更新w的偏導數、b的偏導數、w和b的函數\n",
        "# bgrad即w的偏導數，bgrad即b的偏導數，xdata即輸入的x訓練數據組，ydata即輸入的y訓練數據組，wdata即設定的w0-w8，bdata即設定的b\n",
        "def update_b_grad_and_w_grad_and_w_and_b(bgrad, wgrad, xdata, ydata, wdata, bdata):\n",
        "\t# xdata的數據組規模數量表示了我們用多大規模的數據來更新一次w和b的偏導數，這就是BGD、SGD、MBGD之間的區別\n",
        "\tfor n, x in enumerate(xdata):\n",
        "\t\tbgrad = bgrad - 2.0 * (\n",
        "\t\t\t\tydata[n] - bdata - wdata[0] * x[0] - wdata[1] * x[1] - wdata[2] * x[2] - wdata[3] * x[3] - wdata[4] *\n",
        "\t\t\t\tx[4] - wdata[5] * x[5]) * 1.0\n",
        "\t\tfor m in range(6):\n",
        "\t\t\twgrad[m] = wgrad[m] - 2.0 * (\n",
        "\t\t\t\t\tydata[n] - bdata - wdata[0] * x[0] - wdata[1] * x[1] - wdata[2] * x[2] - wdata[3] * x[3] - wdata[\n",
        "\t\t\t\t4] *\n",
        "\t\t\t\t\tx[4] - wdata[5] * x[5]) * x[m]\n",
        "\t# 用偏導數乘以學習速率來更新w和b\n",
        "\tbdata = bdata - lrb * bgrad\n",
        "\tfor m in range(6):\n",
        "\t\twdata[m] = wdata[m] - lrw * w_grad[m]\n",
        "\treturn bdata, wdata\n",
        "\n",
        "\n",
        "# 存儲訓練得出的最好的w和b值，並記錄此時的training error\n",
        "b_train_best = 1\n",
        "w_train_best = []\n",
        "training_error = 10000\n",
        "\n",
        "for i in range(iteration):\n",
        "\tx_train_minibatch = train_X[0:200]\n",
        "\ty_train_minibatch = train_y[0:200]\n",
        "\tb_temp, w_temp = update_b_grad_and_w_grad_and_w_and_b(b_grad, w_grad, x_train_minibatch, y_train_minibatch, w, b)\n",
        "\t# 每訓練一次存儲更新的w和b\n",
        "\t# 並且將更新的w和b的值賦值給w和b變量\n",
        "\t# 也就是說每訓練一次更新了4個變量：b_grad、w_grad、w、b，b_grad和w_grad直接在函數體內更新，w和b要記錄更新後的值，所以作爲函數返回值\n",
        "\t# 在下面這句更新\n",
        "\tb, w = b_temp, w_temp\n",
        "\tloss_history.append(loss_function1(x_train_minibatch, y_train_minibatch, w_temp, b_temp))\n",
        "\tif loss_history[-1] < training_error:\n",
        "\t\ttraining_error = loss_history[-1]\n",
        "\t\tb_train_best = b_temp\n",
        "\t\tw_train_best.clear()\n",
        "\t\t[w_train_best.append(i) for i in w_temp]\n",
        "\tb_history.append(b_temp)\n",
        "\tw_history.append(w_temp)\n",
        "# 得到訓練出來的最佳w、b、Loss\n",
        "print(training_error, b_train_best, w_train_best)\n",
        "\n",
        "print(\"MBGDlinear的MAE是{}\".format(loss_function1(test_X, test_y, w_train_best, b_train_best)))#mae"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.855621448654821 -0.08280665925413777 [0.20961764415134249, -0.01298784752729445, -0.05832356771125835, -0.257609569255654, 0.235177384366057, 0.8870157251734297]\n",
            "MBGDlinear的MAE是16.331761524603248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh9s_wNSIiwB"
      },
      "source": [
        "##心得\n",
        "\n",
        "\n",
        "\n",
        "*  小結:MBGD的效果與BGD較相近,而且可以跑比較快\n",
        "\n",
        "\n",
        "*   要熟悉演算法套件還有他們的特性\n",
        "*   多練習迴圈&python\n",
        "*  想想看前處理還可以怎麼做(大學問)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "> 再試看看不同方法!\n",
        "\n"
      ]
    }
  ]
}